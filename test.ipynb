{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Visible devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2622\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "input_dir = \"./data/annotation_JPG_process/\"\n",
    "target_dir = './data/annotation_masked_edited_320'\n",
    "\n",
    "\n",
    "img_size = (320, 320)\n",
    "num_classes = 9\n",
    "pre_split = False\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".JPG\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".npy\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "\n",
    "val_samples = 600\n",
    "seed = 42\n",
    "random.Random(seed).shuffle(input_img_paths)\n",
    "random.Random(seed).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "matching_paths = []\n",
    "for input_path in train_input_img_paths:\n",
    "    input_filename = os.path.splitext(input_path)[0].split('/')[-1]  \n",
    "    for target_path in train_target_img_paths:\n",
    "        target_filename = os.path.splitext(target_path)[0].split('/')[-1]  \n",
    "        if input_filename == target_filename:\n",
    "            matching_paths.append((input_path, target_path))\n",
    "            break\n",
    "        \n",
    "train_input_img_paths, train_target_img_paths = zip(*matching_paths)\n",
    "train_input_img_paths = list(train_input_img_paths)\n",
    "train_target_img_paths = list(train_target_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def convertimg(img_paths,img_size,is_training):\n",
    "    x = []\n",
    "    for j, path in enumerate(img_paths):\n",
    "\n",
    "        img = np.array(Image.open(path))\n",
    "        img = img[:,:,:3]\n",
    "        img = Image.fromarray(img)     \n",
    "        img = np.array(img.resize(img_size))\n",
    "        randn = round(random.uniform(0,1)*100)\n",
    "        x.append(img)\n",
    "    return np.array(x)\n",
    "\n",
    "# classification label \n",
    "def extract_label(path):\n",
    "    label_df = pd.read_csv(\"./2022-OCT-Seg-Data/path_label_total.csv\")\n",
    "    file_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    label = label_df[label_df[\"path\"] == file_name][\"label\"].values[0]\n",
    "    if label == \"CRVO\":\n",
    "        label = 0\n",
    "    elif label == \"CSC\":\n",
    "        label = 1\n",
    "    elif label == \"DM\":\n",
    "        label = 2\n",
    "    elif label == \"ERM\":\n",
    "        label = 3\n",
    "    elif label == \"MH\":\n",
    "        label = 4\n",
    "    elif label == \"Normal\":\n",
    "        label = 5\n",
    "    elif label == \"PCV\":\n",
    "        label = 6\n",
    "    elif label == \"RAP\":\n",
    "        label = 7\n",
    "    elif label == \"wetAMD\":\n",
    "        label = 8\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "def seg_label(path, img_size=(320,320)):\n",
    "    path = path.replace('tif', 'npy')\n",
    "    img = np.load(path)\n",
    "    y = img.astype('int').reshape(img_size + (1,))\n",
    "\n",
    "    return y\n",
    "\n",
    "def extract_erm_label(path):\n",
    "    erm_data = pd.read_csv(\"./erm_data.txt\")\n",
    "    file_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    erm_data = np.array(erm_data['fname'])\n",
    "    if file_name in erm_data:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 600\n",
    "\n",
    "\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "\n",
    "val_img = convertimg(val_input_img_paths, img_size, False)\n",
    "\n",
    "n_classes = 9\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "########### clf label ############\n",
    "train_cls_label = []\n",
    "val_cls_label = []\n",
    "train_seg_label = []\n",
    "val_seg_label = []\n",
    "train_erm_label = []\n",
    "val_erm_label = []\n",
    "\n",
    "\n",
    "for path in val_target_img_paths:\n",
    "    val_seg_label.append(seg_label(path))\n",
    "\n",
    "\n",
    "for path in val_input_img_paths:\n",
    "    val_cls_label.append(extract_label(path))\n",
    "\n",
    "for path in val_input_img_paths:\n",
    "    val_erm_label.append(extract_erm_label(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "seg_model = sm.Unet('resnet101', classes=9, activation='softmax', input_shape=(320, 320, 3), encoder_weights='imagenet', encoder_freeze=False)\n",
    "\n",
    "\n",
    "last_layer_name = 'seg_output'\n",
    "seg_model.layers[-1]._name = last_layer_name\n",
    "\n",
    "\n",
    "seg_model = tf.keras.models.Model(inputs=seg_model.inputs, outputs=seg_model.outputs, name=seg_model.name)\n",
    "\n",
    "clf_model = tf.keras.Model(inputs=seg_model.input, outputs=seg_model.get_layer('relu1').output)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(clf_model.output)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(9, activation='softmax', name = \"cls_output\")(x)\n",
    "clf_model = tf.keras.Model(inputs=clf_model.input, outputs=x)\n",
    "\n",
    "# ERM model\n",
    "# Define a binary classification head for predicting the train_erm_label value\n",
    "erm_model = tf.keras.Model(inputs=seg_model.input, outputs=seg_model.get_layer('relu1').output)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(erm_model.output)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid', name='erm_output')(x)\n",
    "erm_model = tf.keras.Model(inputs=erm_model.input, outputs=x)\n",
    "\n",
    "\n",
    "# Define inputs and outputs\n",
    "seg_inputs = seg_model.input\n",
    "clf_inputs = clf_model.input\n",
    "erm_inputs = erm_model.input\n",
    "seg_output = seg_model.output\n",
    "cls_output = clf_model.output\n",
    "erm_output = erm_model.output\n",
    "\n",
    "model = tf.keras.Model(inputs=seg_inputs, outputs=[seg_output, cls_output, erm_output])\n",
    "\n",
    "model.load_weights('./new_ratio_resnet101_320_5_b_4_d_0.5_ermw_0.2_r_1.0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1.) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.)\n",
    "\n",
    "def dice_coef_multilabel(y_true, y_pred, numLabels=9):\n",
    "    dice=0\n",
    "    for index in range(numLabels):\n",
    "        dice -= dice_coef(y_true[:,:,index,:,:], y_pred[:,:,index,:,:])\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 0.5*(1 - dice_coef(y_true, y_pred)) + 0.5*tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
    "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
    "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
    "    tv = tversky(y_true, y_pred)\n",
    "    return K.pow((1 - tv), gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsail/tf_env/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:4212: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    }
   ],
   "source": [
    "val_preds, clf_result, erm_result = model.predict(val_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./2022-OCT-Seg-Data/labels.csv\")\n",
    "label_idx = []\n",
    "labels = ['R32', 'YG23', 'BG02', 'B29', 'E15', 'YR16', 'E43', 'YR00',]\n",
    "df = df[df.copic.isin(labels)]\n",
    "\n",
    "for row in df.values:\n",
    "    r, g, b = row[1], row[2], row[3]\n",
    "    label_idx.append([r,g,b])\n",
    "\n",
    "\n",
    "label_idx.append([0, 0, 0])\n",
    "\n",
    "keys= []\n",
    "for file in val_input_img_paths:\n",
    "    key = file.split('/')[-1].replace('.npy', '')\n",
    "    keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "new_mask_path = './data/annotation_masked_edited_320/'\n",
    "\n",
    "for idx in range(10,20):\n",
    "    source = np.array(Image.open(val_input_img_paths[idx]).resize((320,320)))\n",
    "\n",
    "    key = val_input_img_paths[idx].split('/')[-1].replace('.JPG', '')\n",
    "    source_path = new_mask_path+ key + '.npy'\n",
    "\n",
    "    mask = np.argmax(val_preds[idx], axis=-1)\n",
    "    pred = np.zeros((mask.shape) + (3,))\n",
    "    for i in range(pred.shape[0]):\n",
    "        for j in range(pred.shape[1]):\n",
    "            pred[i, j] = np.array(label_idx)[int(mask[i, j])]\n",
    "    pred = Image.fromarray(pred.astype(np.uint8))\n",
    "    fname = val_input_img_paths[idx].split('/')[-1].replace(\"JPG\", \"tiff\")\n",
    "\n",
    "    mask = np.load(source_path)\n",
    "    target = np.zeros((320,320) + (3,))\n",
    "    for i in range(target.shape[0]):\n",
    "        for j in range(target.shape[1]):\n",
    "            target[i, j] = np.array(label_idx)[int(mask[i, j])]\n",
    "    target = Image.fromarray(target.astype(np.uint8))\n",
    "\n",
    "    val = np.load(val_target_img_paths[idx])\n",
    "    val = keras.utils.to_categorical(val.astype('float32'))\n",
    "    val = keras.utils.to_categorical(mask.astype('float32'))\n",
    "    dice = dice_coef(val, val_preds[idx])\n",
    "\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title('Source')\n",
    "    plt.imshow(source)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('Target')\n",
    "    plt.imshow(target)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(f'Prediction: {dice}')\n",
    "    plt.imshow(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/dsail/jiwonkim/2022-seg/data/annotation_masked_edited_320/00377957_vol_F_54_20150414_L_703_50_189612_bscan_8.npy'\n",
      "[Errno 2] No such file or directory: '/home/dsail/jiwonkim/2022-seg/data/annotation_masked_edited_320/540471201805032013.npy'\n",
      "[Errno 2] No such file or directory: '/home/dsail/jiwonkim/2022-seg/data/annotation_masked_edited_320/00162067_vol_M_57_20200406_L_703_50_219567_bscan_19.npy'\n",
      "[Errno 2] No such file or directory: '/home/dsail/jiwonkim/2022-seg/data/annotation_masked_edited_320/00603644_vol_F_67_20200417_L_703_50_227686_bscan_18.npy'\n",
      "[Errno 2] No such file or directory: '/home/dsail/jiwonkim/2022-seg/data/annotation_masked_edited_320/416023201506242018.npy'\n",
      "[Errno 2] No such file or directory: '/home/dsail/jiwonkim/2022-seg/data/annotation_masked_edited_320/542284201805251018.npy'\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "new_mask_path = './data/annotation_masked_edited_320/'\n",
    "cls_dice = []\n",
    "\n",
    "def dice_coef_multilabel(y_true, y_pred, numLabels=9):\n",
    "    dice=[]\n",
    "    for index in range(numLabels):\n",
    "        res = dice_coef(y_true[:,:,index], y_pred[:,:,index]).numpy()\n",
    "        dice.append(res)\n",
    "    return dice\n",
    "\n",
    "for idx in range(600):\n",
    "    try:\n",
    "        key = val_input_img_paths[idx].split('/')[-1].replace('.JPG', '')\n",
    "        source_path = new_mask_path+ key + '.npy'\n",
    "        mask = np.load(source_path)\n",
    "\n",
    "        val = keras.utils.to_categorical(mask.astype('float32'))\n",
    "        dice = dice_coef_multilabel(val, val_preds[idx])\n",
    "        cls_dice.append(dice)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "new_mask_path = './data/annotation_masked_edited_320/'\n",
    "cls_dice = []\n",
    "\n",
    "for idx in range(600):\n",
    "    try:\n",
    "        key = val_input_img_paths[idx].split('/')[-1].replace('.JPG', '')\n",
    "        source_path = new_mask_path+ key + '.npy'\n",
    "        mask = np.load(source_path)\n",
    "\n",
    "        target_img = np.array(Image.open(val_input_img_paths[idx]).resize((320,320)))\n",
    "\n",
    "        print(mask.shape)\n",
    "        val = keras.utils.to_categorical(mask.astype('float32'))\n",
    "        print(val.shape)\n",
    "        dice = dice_coef_multilabel(val, val_preds[idx])\n",
    "        cls_dice.append(dice)\n",
    "        print(cls_dice)\n",
    "\n",
    "        pred_mask = np.argmax(val_preds[idx], axis=-1)\n",
    "        pred = np.zeros((pred_mask.shape) + (3,))\n",
    "        for i in range(pred.shape[0]):\n",
    "            for j in range(pred.shape[1]):\n",
    "                pred[i, j] = np.array(label_idx)[int(pred_mask[i, j])]\n",
    "        pred = Image.fromarray(pred.astype(np.uint8))\n",
    "\n",
    "        target = np.zeros((320,320) + (3,))\n",
    "        for i in range(target.shape[0]):\n",
    "            for j in range(target.shape[1]):\n",
    "                target[i, j] = np.array(label_idx)[int(mask[i, j])]\n",
    "        target = Image.fromarray(target.astype(np.uint8))\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.title('Source')\n",
    "        plt.imshow(pred)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.title('Target')\n",
    "        plt.imshow(target)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.title('Target img')\n",
    "        plt.imshow(target_img)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label = []\n",
    "for idx in range(600):\n",
    "    val = np.load(val_target_img_paths[idx])\n",
    "    unique_label.append(val)\n",
    "np.unique(unique_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_per_class = np.mean(np.array(cls_dice), axis=0)\n",
    "pd.DataFrame(dice_per_class).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score = []\n",
    "for idx in range(600):\n",
    "        try:\n",
    "                key = val_input_img_paths[idx].split('/')[-1].replace('.JPG', '')\n",
    "                source_path = new_mask_path+ key + '.npy'\n",
    "                mask = np.load(source_path)\n",
    "                val = keras.utils.to_categorical(mask.astype('float32'))\n",
    "\n",
    "                dice = dice_coef(val, val_preds[idx])\n",
    "                dice_score.append(dice)\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "np.array(dice_score).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = []\n",
    "for i in range(len(val_preds)):\n",
    "    pred_label.append(np.argmax(clf_result[i]))\n",
    "cls_label = val_cls_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(val_cls_label))\n",
    "print(len(pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gamma = 1.0\n",
    "\n",
    "def class_report(y_pred,y_true,name):\n",
    "    #multiclass classification report\n",
    "    sensitivty = recall_score(y_true, y_pred, pos_label=1, average='weighted')\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    with open(f'./{name}_cr.txt', 'w') as f:\n",
    "        # print(classification_report(y_true, y_pred), file=f)\n",
    "        print(\"accuracy: \", accuracy_score(y_true, y_pred), file=f)\n",
    "        print(\"sensitivity: \", sensitivty, file=f)\n",
    "        print(\"specificity: \", specificity, file=f)\n",
    "        print(\"precision: \", precision, file=f)\n",
    "        print(\"f1: \", f1, file=f)\n",
    "        print(\"macro_precision: \", macro_precision, file=f)\n",
    "        print(\"macro_recall: \", macro_recall, file=f)\n",
    "        print(\"macro_f1: \", macro_f1, file=f)\n",
    "        print(\"\", file=f)\n",
    "        print(\"Confusion Matrix: \",file=f)\n",
    "        print(confusion_matrix(y_true, y_pred), file=f)\n",
    "\n",
    "# print(classification_report(val_cls_label, pred_label))\n",
    "print(\"accuracy: \", accuracy_score(val_cls_label, pred_label))\n",
    "#classification  result\n",
    "class_report(val_cls_label, pred_label, 'results')\n",
    "\n",
    "#multi label classification report\n",
    "classification_report(val_cls_label, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(val_cls_label, pred_label))\n",
    "print(\"accuracy: \", accuracy_score(val_cls_label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(y_true, y_pred, path='', cname=['CRVO','CSC', 'DM', 'ERM', 'MH', 'Normal', 'PCV', 'RAP', 'wetAMD']):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm = np.round(cm, 2)\n",
    "    df_cm = pd.DataFrame(cm, columns=cname, index=cname)\n",
    "    plt.figure(figsize=(5.5, 4))\n",
    "    sns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(path)\n",
    "\n",
    "draw_confusion_matrix(cls_label, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import Accuracy\n",
    "accuracy = Accuracy()\n",
    "rounded_erm_result = np.round(erm_result)\n",
    "erm_label = val_erm_label\n",
    "pred_label = accuracy(erm_label, rounded_erm_result)\n",
    "print(\"ERM accuracy:\", pred_label.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd511c612f45d288cf3f663fffb070befc9a527188faec91d83b801d4421bb1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
